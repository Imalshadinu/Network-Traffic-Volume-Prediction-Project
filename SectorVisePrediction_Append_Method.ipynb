{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import random\n",
    "import multiprocessing\n",
    "import threading\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from multiprocessing import Manager\n",
    "import itertools\n",
    "import math\n",
    "plt.rcParams['figure.figsize']=(20,10)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "#Read the data file\n",
    "df=pd.read_excel(r'KPI Analysis Result_Query_Result_20210503151726224.xlsx')\n",
    "df=df.iloc[0:2292,:]\n",
    "#df=df.iloc[65782:68086,:]\n",
    "#df=df.iloc[0:3332,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SiteName_array=[]\n",
    "SiteId_array=[]\n",
    "RAT_array=[]\n",
    "CellId_array=[]\n",
    "\n",
    "#Add the columns 'Site Name','Site Id','RAT' and 'Cell Id'\n",
    "#for i in range(65782,68086):\n",
    "for i in range(0,len(df)):\n",
    "    \n",
    "    SiteName = (df['Cell Name'][i].split('-'))[0]\n",
    "    SiteName_array.append(SiteName)\n",
    "    \n",
    "    SiteId = (df['Cell Name'][i].split('-'))[1]\n",
    "    SiteId_array.append(SiteId)\n",
    "    \n",
    "    RAT_CellId = (df['Cell Name'][i].split('-'))[2]\n",
    "    RAT = (RAT_CellId.split('_'))[0]\n",
    "    RAT_array.append(RAT)\n",
    "    CellId = (RAT_CellId.split('_'))[1]\n",
    "    CellId_array.append(CellId)\n",
    "\n",
    "df['Site Name']=SiteName_array\n",
    "df['Site ID']= SiteId_array\n",
    "df['RAT']=RAT_array\n",
    "df['Cell Id']=CellId_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the 'Sector Id' column values to cell groups 1,2,3 and 4\n",
    "SectorId_array=[]\n",
    "#for i in range(65782,68086):\n",
    "for i in range(0,len(df)):\n",
    "    if (df['Cell Id'][i]=='A'or df['Cell Id'][i]=='J'or df['Cell Id'][i]=='P'or df['Cell Id'][i]=='U'):\n",
    "        SectorId_array.append(1)\n",
    "    elif (df['Cell Id'][i]=='B'or df['Cell Id'][i]=='K'or df['Cell Id'][i]=='Q'or df['Cell Id'][i]=='V'):\n",
    "        SectorId_array.append(2)\n",
    "    elif (df['Cell Id'][i]=='C'or df['Cell Id'][i]=='L'or df['Cell Id'][i]=='R'or df['Cell Id'][i]=='W'):\n",
    "        SectorId_array.append(3)\n",
    "    elif (df['Cell Id'][i]=='D'or df['Cell Id'][i]=='M'or df['Cell Id'][i]=='S'or df['Cell Id'][i]=='X'):\n",
    "        SectorId_array.append(4)\n",
    "\n",
    "df['Sector Id'] = SectorId_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the 'Sector Unique Id' column\n",
    "df['Sector Unique Id'] = df['Site ID']+'_'+df['Sector Id'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global result_df\n",
    "result_df = pd.DataFrame()\n",
    "#global avg_result_df\n",
    "avg_result_df = pd.DataFrame()\n",
    "drop_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_pred(j):\n",
    "    t1=time.time()\n",
    "    global result_df\n",
    "    global avg_result_df\n",
    "    global drop_df\n",
    "    #dfs_list = Manager().list()\n",
    "    #global result_final_df\n",
    "    #Get the data frames as groups according to the 'Sector Unique Id'\n",
    "    grouped = df.groupby(df['Sector Unique Id'])\n",
    "    array=[]\n",
    "    #Get the 'Sector Unique Id' values into an array\n",
    "    array=df['Sector Unique Id'].unique()\n",
    "    #Get the data frames as groups according to the 'Sector Unique Id'\n",
    "    g=grouped.get_group(array[j])\n",
    "    sec_uni_id=array[j]\n",
    "    #Group the data frame according to the 'Date'\n",
    "    gr_date = g.groupby(['Date']).agg({'Sector Unique Id':'first','Ish.L.TrafficVolume.DL.GB(GB)': 'sum'}).reset_index()\n",
    "    print(gr_date)\n",
    "    #Check whether the length of the dataframe is below 80% percent(240) of the total data points\n",
    "    if (len(gr_date)<240):\n",
    "        #increase the dropped cell count\n",
    "        #drop_count+=1\n",
    "        #When the length of the dataframe is below 80% percent(240) of the total data points, check whether the sector is a newly turned on sector or not\n",
    "        if gr_date['Date'][len(gr_date)-1]==pd.Timestamp('2021-05-02 00:00:00') :\n",
    "            #calculate the average network traffic volume values of last 5 days\n",
    "            avg_traf_vol = gr_date['Ish.L.TrafficVolume.DL.GB(GB)'].iloc[len(gr_date)-5:len(gr_date)].mean(axis=0)\n",
    "            #Create the dates for next 6 months\n",
    "            date_array = pd.date_range(start='05/03/2021', periods=180, freq='D')\n",
    "            #Apppend the average network traffic volume values for next 180 days\n",
    "            for k in range(0,len(date_array)):\n",
    "                avg_result_df = avg_result_df.append({'ds':date_array[k],'Sector Unique Id':sec_uni_id,'yhat_rescaled':avg_traf_vol}, ignore_index=True)\n",
    "            \n",
    "            print(sec_uni_id+' sector has the data from '+str(gr_date['Date'][0])+'. The sector has new cells and cannot make prediction due to less data.')\n",
    "            #drop_df = drop_df.append({'ds':'Dropped','Sector Unique Id':sec_uni_id,'yhat_rescaled':'Sector is a new one and includes the data from'+str(gr_date['Date'][0])+'. Prediction cannot be done due to less data.'},ignore_index=True)\n",
    "            #drop_df = drop_df.append({'Sector Unique Id':sec_uni_id,'Description':'Cells are new and includes the data from'+str(gr_date[0])+'. Prediction cannot be done due to less data.'},ignore_index=True)\n",
    "            #drop_df.to_excel(drop_cells)\n",
    "        else :\n",
    "            print(sec_uni_id+' sector has '+str(len(gr_date))+'data points. Require to drop due to less data.')\n",
    "            drop_df = drop_df.append({'ds':'Dropped','Sector Unique Id':sec_uni_id,'yhat_rescaled':'Many data points are missing. Prediction cannot be done.'},ignore_index=True)\n",
    "            #drop_df = drop_df.append({'Sector Unique Id':sec_uni_id,'Description':'Many data points are missing. Prediction cannot be done.'},ignore_index=True)\n",
    "            #drop_df.to_excel(drop_cells)\n",
    "    #Identify the '0' value count is greater than the 20% of the total number of data points\n",
    "    elif (gr_date['Ish.L.TrafficVolume.DL.GB(GB)']==0).astype(int).sum(axis=0)>(0.2*len(gr_date)):  #identify the rows having zeros as data values more than 80% of the dataframe\n",
    "        print(sec_uni_id+' Sector has '+str((gr_date['Ish.L.TrafficVolume.DL.GB(GB)']==0).astype(int).sum(axis=0))+' zeros.Require to drop')\n",
    "        drop_df = drop_df.append({'ds':'Dropped','Sector Unique Id':sec_uni_id,'yhat_rescaled':'Many data points have zeros. Prediction cannot be done.'},ignore_index=True)\n",
    "        #drop_for_zero+=1\n",
    "        #drop_df = drop_df.append({'Sector Unique Id':sec_uni_id,'Description':'Many data points have zeros. Prediction cannot be done.'},ignore_index=True)\n",
    "        #drop_df.to_excel(drop_cells)\n",
    "    else:\n",
    "        dataset = gr_date.filter(['Date','Ish.L.TrafficVolume.DL.GB(GB)'])\n",
    "        #Check the missing values in the data frame and put them into an array\n",
    "        zero_index_array=[]\n",
    "        \n",
    "        for i in range(0,len(dataset)):\n",
    "            if (dataset.iloc[i]['Ish.L.TrafficVolume.DL.GB(GB)']==0):\n",
    "                zero_index_array.append(i)\n",
    "                \n",
    "        if len(zero_index_array)>0 :\n",
    "            for k in zero_index_array:\n",
    "                #Check whether the first value in the data frame is zero and replece the zeros with the average of nearby 10 data points\n",
    "                if 0 in zero_index_array :\n",
    "                    dataset.replace(0,dataset.iloc[k+len(zero_index_array)+1:k+len(zero_index_array)+11].mean(axis=0),inplace=True)\n",
    "                    break\n",
    "                #Check whether the last value in the data frame is zero and replece the zeros with the average of nearby 10 data points\n",
    "                elif (len(dataset)-1) in zero_index_array:\n",
    "                    dataset.replace(0,dataset.iloc[k-len(zero_index_array)-9:k-len(zero_index_array)+1].mean(axis=0),inplace=True)\n",
    "                    break\n",
    "                #Replace the zeros by the average of nearby 10 data values\n",
    "                else:\n",
    "                    dataset_sub=(dataset.iloc[k-5:k+6])[(dataset.iloc[k-5:k+6])['Ish.L.TrafficVolume.DL.GB(GB)']!=0]\n",
    "                    dataset.replace(0,dataset_sub.mean(axis=0),inplace=True)\n",
    "        #dataset.replace(0,dataset.mean(axis=0),inplace=True)\n",
    "        dataset.columns = ['ds', 'y']\n",
    "        dataset['ds']= pd.to_datetime(dataset['ds'])\n",
    "        #Make a column to hold the original network traffic volume values\n",
    "        dataset['y_orig']=dataset.y\n",
    "        #Take the log scale of the data values in order to reduce the skewness\n",
    "        dataset['y'] = np.log(dataset['y'])\n",
    "        #acc_train_tot=0\n",
    "        #r2_train_tot=0\n",
    "        #Take the length of initial 80% of the data frame as the initial training data set length\n",
    "        train_initial_len = math.ceil(len(dataset)*0.8)\n",
    "        initial_dataset = dataset\n",
    "        #For predict month by month create a for loop\n",
    "        for k in range(6):\n",
    "            new_dataset=pd.DataFrame()\n",
    "            print(dataset)\n",
    "            #Take the length of initial 80% of the data frame as the training data set length\n",
    "            train_data_len = math.ceil(len(dataset)*0.8)\n",
    "            #Take the length of test data set\n",
    "            test_data_len =len(dataset)-train_data_len\n",
    "            #Assign the training data set\n",
    "            train =dataset.drop(dataset.index[-test_data_len:])\n",
    "            #Parameters which are chosen to tune\n",
    "            params_grid = {    \n",
    "                'changepoint_prior_scale': [0.005,0.001,0.01,0.1,0.5],  \n",
    "                'seasonality_prior_scale': [0.01, 0.1, 1.0, 10.0]\n",
    "            }\n",
    "\n",
    "            grid = ParameterGrid(params_grid)\n",
    "            #Create a dataframe to store the 'MAPE' and 'Parameters'\n",
    "            model_parameters = pd.DataFrame(columns = ['MAPE','Parameters'])\n",
    "        \n",
    "            for p in grid:\n",
    "                \n",
    "                random.seed(0)\n",
    "                #Build the Prophet model for train\n",
    "                train_model =Prophet(seasonality_prior_scale = p['seasonality_prior_scale'],\n",
    "                             changepoint_prior_scale = p['changepoint_prior_scale'],\n",
    "                             interval_width=0.95).add_seasonality(name='monthly',period=30.5,fourier_order=12).add_seasonality(name='dailly',period=1,fourier_order=15).add_seasonality(name='weekly',period=7,fourier_order=20).add_seasonality(name='quarterly',period=365.25/4,fourier_order=5) \n",
    "                #Fit the Prophet model\n",
    "                train_model.fit(train)\n",
    "                #Make the future data frame for predict\n",
    "                train_forecast = train_model.make_future_dataframe(periods=test_data_len, freq='D',include_history = False)\n",
    "                #Get the predictions\n",
    "                train_forecast = train_model.predict(train_forecast)\n",
    "                #Get the predicted values in the original scale\n",
    "                train_forecast['yhat_rescaled'] = np.exp(train_forecast['yhat'])\n",
    "                test=train_forecast[['ds','yhat_rescaled']]\n",
    "                #Take the Mean Absolute Percentage Error\n",
    "                MAPE = mean_absolute_percentage_error(dataset['y_orig'][-test_data_len:],abs(test['yhat_rescaled']))\n",
    "                #Assign the values 'MAPE' values with parameters in to the data frame\n",
    "                model_parameters = model_parameters.append({'MAPE':MAPE,'Parameters':p},ignore_index=True)\n",
    "        \n",
    "            #Sort the data frame according to the 'MAPE' value\n",
    "            parameters = model_parameters.sort_values(by=['MAPE'])\n",
    "            parameters = parameters.reset_index(drop=True)\n",
    "            #Get the best parameters\n",
    "            best_params = parameters['Parameters'][0]\n",
    "            print('Best Parameters for'+sec_uni_id+':',best_params)\n",
    "            #Build the Prophet model\n",
    "            model = Prophet(interval_width=0.95, changepoint_prior_scale=best_params['changepoint_prior_scale'], seasonality_prior_scale = best_params['seasonality_prior_scale']).add_seasonality(name='monthly',period=30.5,fourier_order=12).add_seasonality(name='dailly',period=1,fourier_order=15).add_seasonality(name='weekly',period=7,fourier_order=20).add_seasonality(name='quarterly',period=365.25/4,fourier_order=5)\n",
    "            #Fit the model with the train data\n",
    "            model.fit(train) \n",
    "            #Make the future data frame for test data set\n",
    "            future = model.make_future_dataframe(periods=test_data_len)\n",
    "            #Get the predicted values\n",
    "            forecast = model.predict(future)\n",
    "            #Build a matrix to hold the predicted values and original values\n",
    "            metric_df = forecast.set_index('ds')[['yhat']].join(dataset.set_index('ds').y_orig).reset_index()\n",
    "            #Get the predicted values in the original scale\n",
    "            metric_df['yhat_rescaled'] = np.exp(metric_df['yhat'])\n",
    "            print(metric_df)\n",
    "            #Calculate the r2_score\n",
    "            r2score=r2_score(metric_df['y_orig'], metric_df['yhat_rescaled'])\n",
    "            print('r2_score =',r2score)\n",
    "            #r2_train_tot+=r2score\n",
    "            y_true = metric_df['y_orig'][-test_data_len:]\n",
    "        \n",
    "            y_pred = metric_df['yhat_rescaled'][-test_data_len:]\n",
    "            #Get the mean absolute error \n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            print('MAE: %.3f' % mae)\n",
    "            #Get the mean absolute percentage error \n",
    "            mape=np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "            print('Mean Absolute Percentage Error (MAPE) =', mape)\n",
    "            #Get the accuracy\n",
    "            Acc= 100-mape\n",
    "            print('Accuracy of the prediction=',Acc) \n",
    "            #Take the sum of the accutracy values in a sector data frame\n",
    "            #acc_train_tot+=Acc\n",
    "            #Make the future data frame for next 6 months' dates\n",
    "            future_6 = model.make_future_dataframe(periods=(test_data_len+30))\n",
    "            #Get the predictions\n",
    "            forecast_6 = model.predict(future_6)\n",
    "            \n",
    "            metric_df_2 = forecast_6.set_index('ds')[['yhat', 'yhat_lower','yhat_upper']].join(dataset.set_index('ds').y_orig).reset_index()\n",
    "            #Get the predicted values in the original scale\n",
    "            metric_df_2['yhat_rescaled'] = np.exp(metric_df_2['yhat'])\n",
    "            #Put the 'Sector Unique Id' in to a column in the matrix\n",
    "            for i in range (0,len(g)):\n",
    "                metric_df_2['Sector Unique Id'] =sec_uni_id\n",
    "            \n",
    "            \n",
    "        #metric_df_2.set_index('ds', inplace=True)\n",
    "        \n",
    "            print(metric_df_2[['yhat_rescaled','y_orig']])\n",
    "        \n",
    "            #model.plot(forecast_6);\n",
    "            #plt.savefig('Prediction_Sector_Vise_Append/PredFinalVisu_Append/'+sec_uni_id+'_next_6_month_plot1.png')\n",
    "        \n",
    "            #model.plot_components(forecast_6);\n",
    "            #plt.savefig('Prediction_Sector_Vise_Append/PredFinalVisu_Append/'+sec_uni_id+'_next_6_month_plot2.png') \n",
    "            #plt.show()\n",
    "       \n",
    "            #Append the results with the training data set\n",
    "            new_dataset=new_dataset.append(metric_df_2[['ds','yhat','yhat_rescaled']][-30:])\n",
    "            new_dataset.columns=['ds','y','y_orig']\n",
    "            print(new_dataset)\n",
    "        \n",
    "            dataset = dataset.append(new_dataset, ignore_index=True)\n",
    "            print(dataset)\n",
    "            \n",
    "        #Average training accuracy for the sector\n",
    "        #avg_train_acc=acc_train_tot/6\n",
    "        #print('Average accuracy:',avg_train_acc)\n",
    "        #Average r2_score for the sector\n",
    "        #avg_train_r2=r2_train_tot/6\n",
    "        #print('Average r2_score :',avg_train_r2)\n",
    "        #Sum of the r2_scores of sectors\n",
    "        #r2_tot+=avg_train_r2\n",
    "        #Sum of the accuracies of sectors\n",
    "        #acc_tot+=avg_train_acc\n",
    "        \n",
    "        #Set the 'ds' column as the index column\n",
    "        metric_df_2.set_index('ds', inplace=True)\n",
    "        #metric_df_2[['yhat_rescaled','y_orig']][train_initial_len:].to_excel('Prediction_Sector_Vise_Append/PredFinalData2/'+sec_uni_id+'.xlsx')\n",
    "        #metric_df_2[['yhat_rescaled']][train_initial_len:].to_excel('Prediction_Sector_Vise_Append/PredFinalData/'+sec_uni_id+'.xlsx')\n",
    "        #Results of the prediction were append to the data frame\n",
    "        result_df = result_df.append(metric_df_2[['Sector Unique Id','yhat_rescaled']][train_initial_len:])\n",
    "        ##########result_df = result_df.append({'Date':date_array[j],'Cell Name':name,'Predicted Traffic Volume':metric_df_2[['yhat_rescaled']]},ignore_index=True)\n",
    "        #result_df.to_excel(predictions_6months)\n",
    "        \n",
    "        #Plot of 'Actual Value' and 'Predicted Value'\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.plot(metric_df_2.y_orig,color='b',label='Actual Value')\n",
    "        ax1.plot(metric_df_2.yhat_rescaled,color='r',label='Predicted Value')\n",
    "        ax1.fill_between(metric_df_2.index, np.exp(metric_df_2['yhat_upper']), np.exp(metric_df_2['yhat_lower']), alpha=0.5, color='lightblue')\n",
    "        ax1.set_ylabel('Network Traffic Volume')\n",
    "        ax1.set_xlabel('Date')\n",
    "        ax1.legend() #get the legend\n",
    "        plt.savefig('Prediction_Sector_Vise_Append/PredFinalvisu_6 month/'+sec_uni_id+'__6_month_orig_plot1.png')\n",
    "        plt.show()\n",
    "        \n",
    "        #Plot of train data, test data and predicted values\n",
    "        plt.plot(metric_df_2['y_orig'][0:train_initial_len], label='Training',color='g')\n",
    "        plt.plot(metric_df_2['y_orig'][train_initial_len:len(initial_dataset)], label='Test',color='r')\n",
    "        plt.plot(metric_df_2['yhat_rescaled'][train_initial_len:], label='Predicted',color='b')\n",
    "        plt.legend()\n",
    "        plt.savefig('Prediction_Sector_Vise_Append/PredFinalTrainTestPred_6_month/'+sec_uni_id+'_plot1.png')\n",
    "        plt.show()\n",
    "        #Increase the number of predicted sectors\n",
    "        #count+=1\n",
    "        t2=time.time()\n",
    "        print('Time for '+sec_uni_id+' cell :',(t2-t1))\n",
    "\n",
    " \n",
    "    if len(avg_result_df)>0:\n",
    "        #Set the index column of the 'avg_result_df' data frame\n",
    "        avg_result_df=avg_result_df.set_index('ds')\n",
    "        #Combine the two data frames having the predictions\n",
    "        result_df=pd.concat([result_df,avg_result_df],axis=0)\n",
    "    if len(drop_df)>0:\n",
    "        #Set the index column of the 'avg_result_df' data frame\n",
    "        drop_df=drop_df.set_index('ds')\n",
    "        #Combine the two data frames having the predictions\n",
    "        result_df=pd.concat([result_df,drop_df],axis=0) \n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the multiprocessing method\n",
    "def run_multiprocessing(func, tasks, n_processors):\n",
    "    with Pool(processes=n_processors) as pool:\n",
    "        #return pool.starmap(func, iterable=tasks)\n",
    "        result_final_df = pool.map(func, tasks)\n",
    "        \n",
    "        print(pd.concat(result_final_df,axis=0))\n",
    "\n",
    "        return pd.concat(result_final_df,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    array=[]\n",
    "    #Get the 'Sector Unique Id' values into an array\n",
    "    array=df['Sector Unique Id'].unique()\n",
    "    result_final_df = pd.DataFrame()\n",
    "    #global drop_df\n",
    "    #drop_df = pd.DataFrame(columns = ['Sector Unique Id','Description'])\n",
    "    avg_result_df_final=pd.DataFrame()\n",
    "    drop_df = pd.DataFrame()\n",
    "    n_processors=multiprocessing.cpu_count()\n",
    "    x_ls = list(range(0,len(array)))\n",
    "    out = run_multiprocessing(cal_pred, x_ls, n_processors)\n",
    "     \n",
    "    predictions_6months = pd.ExcelWriter('Prediction_Sector_Vise_Append/TrafficVolumePredictions_Direct_threding.xlsx', engine='xlsxwriter')\n",
    "    #drop_cells = pd.ExcelWriter('Prediction_Sector_Vise_Append/DetailsOfDropCells_Direct_threading.xlsx', engine='xlsxwriter')\n",
    "    #Write the results into the excel file\n",
    "    out.to_excel(predictions_6months)\n",
    "\n",
    "    predictions_6months.save()\n",
    "    #drop_cells.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    freeze_support()   # required to use multiprocessing\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time:  830.6626844406128\n"
     ]
    }
   ],
   "source": [
    "end=time.time()\n",
    "print('Total Time: ',(end-start)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
